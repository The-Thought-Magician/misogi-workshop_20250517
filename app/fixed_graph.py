"""
LangGraph Outfit Recommender - Core Logic

This is a beginner-friendly implementation of a LangGraph-powered AI stylist that:
1. Collects user information (height, gender, location, occasion, mood)
2. Fetches weather data for the location
3. Generates personalized outfit recommendations using LLM
4. Collects user feedback and iterates up to 5 times until rating >= 7
5. Provides transparent logging of all decisions and steps

LangGraph Concepts Used:
- StateGraph: Manages the flow and state between nodes
- Nodes: Individual functions that perform specific tasks
- Edges: Define the flow between nodes
- Conditional Edges: Allow branching based on conditions
- State: Shared data structure that flows through all nodes
"""

import os
import sys
from typing import TypedDict, Annotated, List, Dict, Any
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
# Ensure the project root is on sys.path when running this file directly
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.append(parent_dir)

from app.tavily_weather_service import get_weather_for_location, AVAILABLE_LOCATIONS

# Load environment variables
load_dotenv()

# Ensure API key is available
if "OPENAI_API_KEY" not in os.environ:
    raise ValueError("OPENAI_API_KEY not found. Please set it in a .env file.")

# --- LLM Setup ---
# Initialize LLM (OPENAI_API_KEY is loaded from environment via dotenv)
LLM = ChatOpenAI(temperature=0, model="gpt-4o-mini")

# --- State Definition ---
class OutfitState(TypedDict):
    """
    State schema for the outfit recommendation graph.
    
    This TypedDict defines the structure of data that flows between all nodes.
    Each field is annotated to explain its purpose for learning purposes.
    
    In LangGraph, the state is automatically passed between nodes and 
    any updates made by a node are merged back into the shared state.
    """
    # User inputs (collected from frontend)
    height: Annotated[str, "User height (e.g., '5'10\"' or '170cm')"]
    gender: Annotated[str, "User gender (Male/Female/Other)"]
    location: Annotated[str, "User location (Indian state for weather context)"]
    occasion: Annotated[str, "Occasion type (Party/Date/Formal/Casual/Vacation)"]
    mood: Annotated[str, "Desired style mood (Confident/Minimal/Bold/etc.)"]
    
    # Processed data (generated by nodes)
    weather: Annotated[Dict[str, Any], "Weather info fetched from API"]
    recommendation: Annotated[str, "AI-generated outfit recommendation text"]
    rating: Annotated[int, "User feedback rating (1-10 scale)"]
    attempts: Annotated[int, "Current number of recommendation attempts"]
    max_attempts: Annotated[int, "Maximum allowed attempts (default: 5)"]
    result_message: Annotated[str, "Final success/failure message"]
    log: Annotated[List[str], "Step-by-step execution log for transparency"]

# --- Graph Nodes ---
# Each node is a function that takes the current state and returns updates to be merged

def get_weather(state: OutfitState) -> Dict:
    """
    Node 1: Fetch Weather Data
    
    This node demonstrates how to:
    1. Extract data from the shared state
    2. Call external APIs (weather service)
    3. Update the state with new information
    4. Add transparent logging
    
    Args:
        state: Current state containing user's location
    
    Returns:
        Dict with weather data and updated log
    """
    location = state.get("location", "Default")
    
    # Call our weather service (which uses Tavily API or fallback data)
    weather_info = get_weather_for_location(location)
    
    # Create detailed log entry for transparency
    source = weather_info.get("data_source", "fallback")
    log_entry = f"✅ Node: get_weather - Location: {location}, Weather: {weather_info['temp_c']}°C, {weather_info['condition']} (Source: {source})"
    logs = state.get("log", [])
    
    # Return only the fields we want to update in the state
    return {"weather": weather_info, "log": logs + [log_entry]}

def generate_outfit(state: OutfitState) -> Dict:
    """
    Node 2: Generate Outfit Recommendation
    
    This node demonstrates how to:
    1. Access multiple fields from the state
    2. Call an LLM with structured prompts
    3. Handle errors gracefully
    4. Track attempt counters
    
    Args:
        state: Current state with user info and weather data
    
    Returns:
        Dict with AI-generated recommendation and updated counters
    """
    # Extract all required information from the shared state
    height = state["height"]
    gender = state["gender"]
    location = state["location"]
    occasion = state["occasion"]
    mood = state["mood"]
    weather = state["weather"]
    
    # Increment attempt counter (starts at 0, so first attempt becomes 1)
    attempt = state.get("attempts", 0) + 1
    
    # Construct a detailed prompt for the LLM
    # This is a key part of prompt engineering for better results
    prompt = f"""You are an expert fashion stylist AI. Generate a specific and actionable outfit recommendation.

👤 User Profile:
- Height: {height}
- Gender: {gender}

🎯 Context:
- Location: {location}
- Occasion: {occasion}
- Desired Mood: {mood}
- Current Weather: {weather['temp_c']}°C, {weather['condition']}

📝 Instructions:
- Be creative but practical
- Consider the weather conditions
- Match the occasion and desired mood
- Include specific clothing items, colors, and accessories
- Keep it concise but detailed
{f'- This is attempt #{attempt}, try a different style if possible' if attempt > 1 else ''}

Outfit Recommendation:"""

    # Log the LLM call for transparency
    log_entry = f"🤖 Node: generate_outfit - Attempt: {attempt} - Calling LLM for {gender}, {occasion}, {mood} outfit"
    logs = state.get("log", [])
    logs.append(log_entry)
    
    # Make the LLM call with error handling
    try:
        response = LLM.invoke([HumanMessage(content=prompt)])
        content = response.content
        # content can be a string or a list of content parts depending on the model
        if isinstance(content, str):
            recommendation = content.strip()
        else:
            # Extract text fields from content parts
            parts = []
            for part in content:
                if isinstance(part, str):
                    parts.append(part)
                elif isinstance(part, dict):
                    if part.get("type") == "text":
                        parts.append(part.get("text", ""))
            recommendation = "\n".join([p for p in parts if p]).strip()
        log_entry_resp = f"✅ LLM Response received (length: {len(recommendation)} chars)"
    except Exception as e:
        recommendation = f"Sorry, I couldn't generate a recommendation due to an LLM error: {str(e)}"
        log_entry_resp = f"❌ LLM Error: {str(e)}"
    
    # Return the updated fields
    return {
        "recommendation": recommendation, 
        "attempts": attempt,
        "log": logs + [log_entry_resp]
    }

def check_rating(state: OutfitState) -> str:
    """
    Conditional Edge Function: Decision Logic
    
    This is a special function that determines which node to execute next.
    It demonstrates LangGraph's conditional routing capability.
    
    Decision Logic:
    - If rating >= 7: User is satisfied → Generate final result
    - If max attempts reached: Stop trying → Generate final result  
    - Otherwise: Try again → Generate new outfit
    
    Args:
        state: Current state with rating and attempt info
    
    Returns:
        String indicating next node name ("generate_result" or "generate_outfit")
    """
    rating = state.get("rating", 0)
    attempts = state.get("attempts", 0)
    max_attempts = state.get("max_attempts", 5)
    
    # Log the decision-making process for transparency
    logs = state.get("log", [])
    log_entry = f"🤔 Decision Point: Rating={rating}/10, Attempts={attempts}/{max_attempts}"
    logs.append(log_entry)
    
    # Apply decision logic
    if rating >= 7:
        decision = "generate_result"
        log_entry_decision = f"✅ DECISION: Rating satisfactory ({rating} ≥ 7) → Generating final result"
    elif attempts >= max_attempts:
        decision = "generate_result"
        log_entry_decision = f"⏰ DECISION: Max attempts reached ({attempts}/{max_attempts}) → Generating final result"
    else:
        decision = "generate_outfit"
        log_entry_decision = f"🔄 DECISION: Rating too low ({rating} < 7), trying again (attempt {attempts+1}/{max_attempts})"
    
    # Update the log in the state (we modify state directly since this is a conditional function)
    logs.append(log_entry_decision)
    state["log"] = logs
    
    # Return the name of the next node to execute
    return decision

def generate_result(state: OutfitState) -> Dict:
    """
    Node 3: Generate Final Result
    
    This is the terminal node that creates the final message.
    It demonstrates how to:
    1. Analyze the final state
    2. Create appropriate success/failure messages
    3. Provide a satisfying conclusion to the user journey
    
    Args:
        state: Final state with all attempts and ratings
    
    Returns:
        Dict with the final result message
    """
    rating = state.get("rating", 0)
    attempts = state.get("attempts", 0)
    max_attempts = state.get("max_attempts", 5)
    recommendation = state.get("recommendation", "No recommendation generated")
    
    # Create different messages based on success/failure
    if rating >= 7:
        result = f"""🎉 Perfect outfit found after {attempts} attempt(s)! 

⭐ Final Rating: {rating}/10

👕 Final Recommendation:
{recommendation}

Thank you for using the AI Outfit Recommender! Style on! ✨"""
    else:
        result = f"""💔 We tried our best but couldn't find the perfect outfit after {max_attempts} attempts.

⭐ Best Rating Achieved: {rating}/10

👕 Last Recommendation:
{recommendation}

Don't worry! Fashion is subjective. Feel free to try again with different preferences! 🔄"""
    
    # Final log entry
    logs = state.get("log", [])
    log_entry = f"🏁 Node: generate_result - Journey complete! Created final message for user."
    
    return {
        "result_message": result,
        "log": logs + [log_entry]
    }

# --- Graph Definition ---
def create_graph():
    """
    Creates the LangGraph workflow.
    
    This function demonstrates the core LangGraph concepts:
    1. StateGraph: The main graph container
    2. Nodes: Individual processing units
    3. Edges: Define the flow between nodes
    4. Conditional Edges: Allow branching logic
    5. Entry Points: Where execution starts
    6. END: Where execution terminates
    
    Returns:
        StateGraph: The compiled workflow ready for execution
    """
    
    # Step 1: Create a StateGraph with our state schema
    workflow = StateGraph(OutfitState)
    
    # Step 2: Add all the nodes (processing functions)
    workflow.add_node("get_weather", get_weather)          # Fetch weather data
    workflow.add_node("generate_outfit", generate_outfit)  # Generate AI recommendation
    workflow.add_node("generate_result", generate_result)  # Create final message
    
    # Step 3: Define the entry point (first node to execute)
    workflow.set_entry_point("get_weather")
    
    # Step 4: Add regular edges (fixed transitions)
    workflow.add_edge("get_weather", "generate_outfit")    # Weather → Outfit generation
    
    # Step 5: Add conditional edges (dynamic transitions based on logic)
    # After generating an outfit, check_rating() decides what happens next
    workflow.add_conditional_edges(
        "generate_outfit",           # Source node
        check_rating,                # Decision function
        {
            "generate_outfit": "generate_outfit",  # If retry needed → back to outfit generation
            "generate_result": "generate_result"   # If satisfied/max attempts → final result
        }
    )
    
    # Step 6: Connect final node to END (terminates execution)
    workflow.add_edge("generate_result", END)
    
    return workflow

# --- Graph Compilation ---
# Create and compile the graph for execution
app_graph = create_graph()
app = app_graph.compile()

# The compiled 'app' is what we'll use in the UI to execute the workflow

# --- Example Usage for Testing ---
if __name__ == "__main__":
    """
    Test the graph with sample data to verify it works correctly.
    This is useful for debugging and understanding the flow.
    """
    print("🧥 Testing the LangGraph Outfit Recommendation System...")
    print("=" * 60)
    
    # Create test input matching our state schema
    test_input = {
        "height": "5'10\"",
        "gender": "Male", 
        "location": "Mumbai",
        "occasion": "Party",
        "mood": "Confident",
        "rating": 8,  # High rating to test success path
        "attempts": 0,
        "max_attempts": 3,
        "log": ["🚀 Starting test run..."],
        "weather": {},
        "recommendation": "",
        "result_message": ""
    }
    
    # Execute the graph
    print(f"📊 Input: {test_input['gender']} from {test_input['location']} needs {test_input['occasion']} outfit")
    result = app.invoke(test_input)
    
    # Display results
    print(f"\n✅ Execution completed!")
    print(f"📈 Attempts made: {result.get('attempts')}")
    print(f"⭐ Final rating: {result.get('rating')}")
    print(f"📝 Log entries generated: {len(result.get('log', []))}")
    
    print(f"\n👕 Final Recommendation:")
    print("-" * 40)
    print(result.get("recommendation", "No recommendation generated"))
    
    print(f"\n📋 Execution Log:")
    print("-" * 40)
    for i, log_entry in enumerate(result.get("log", []), 1):
        print(f"{i:2d}. {log_entry}")
        
    print(f"\n🏁 Final Message:")
    print("-" * 40) 
    print(result.get("result_message", "No final message"))
    
    print("\n" + "=" * 60)
    print("🎉 Test completed! The graph is working correctly.") 